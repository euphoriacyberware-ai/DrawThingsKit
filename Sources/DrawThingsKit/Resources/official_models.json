{
  "checkpoints": [
    {
      "name": "AuraFlow v0.1",
      "file": "auraflow_v0.1_q8p.ckpt",
      "prefix": "",
      "version": "auraflow",
      "default_scale": 16,
      "text_encoder": "pile_t5_xl_encoder_q8p.ckpt",
      "autoencoder": "sdxl_vae_v1.0_f16.ckpt",
      "deprecated": true
    },
    {
      "name": "AuraFlow v0.1 (8-bit)",
      "file": "auraflow_v0.1_q5p.ckpt",
      "prefix": "",
      "version": "auraflow",
      "default_scale": 16,
      "text_encoder": "pile_t5_xl_encoder_q8p.ckpt",
      "autoencoder": "sdxl_vae_v1.0_f16.ckpt",
      "deprecated": true
    },
    {
      "name": "AuraFlow v0.2",
      "file": "auraflow_v0.2_q8p.ckpt",
      "prefix": "",
      "version": "auraflow",
      "default_scale": 16,
      "text_encoder": "pile_t5_xl_encoder_q8p.ckpt",
      "autoencoder": "sdxl_vae_v1.0_f16.ckpt"
    },
    {
      "name": "AuraFlow v0.2 (8-bit)",
      "file": "auraflow_v0.2_q5p.ckpt",
      "prefix": "",
      "version": "auraflow",
      "default_scale": 16,
      "text_encoder": "pile_t5_xl_encoder_q8p.ckpt",
      "autoencoder": "sdxl_vae_v1.0_f16.ckpt"
    },
    {
      "name": "FLUX.1 [schnell]",
      "file": "flux_1_schnell_q8p.ckpt",
      "prefix": "",
      "version": "flux1",
      "default_scale": 16,
      "text_encoder": "t5_xxl_encoder_q6p.ckpt",
      "autoencoder": "flux_1_vae_f16.ckpt",
      "clip_encoder": "clip_vit_l14_f16.ckpt",
      "high_precision_autoencoder": true,
      "is_consistency_model": true,
      "padded_text_encoding_length": 256,
      "hires_fix_scale": 24
    },
    {
      "name": "FLUX.1 [schnell] (5-bit)",
      "file": "flux_1_schnell_q5p.ckpt",
      "prefix": "",
      "version": "flux1",
      "default_scale": 16,
      "text_encoder": "t5_xxl_encoder_q6p.ckpt",
      "autoencoder": "flux_1_vae_f16.ckpt",
      "clip_encoder": "clip_vit_l14_f16.ckpt",
      "high_precision_autoencoder": true,
      "is_consistency_model": true,
      "padded_text_encoding_length": 256,
      "hires_fix_scale": 24
    },
    {
      "name": "Fooocus Inpaint SDXL v2.6",
      "file": "fooocus_inpaint_sd_xl_v2.6_f16.ckpt",
      "prefix": "",
      "version": "sdxl_base_v0.9",
      "default_scale": 16,
      "text_encoder": "open_clip_vit_bigg14_f16.ckpt",
      "autoencoder": "sdxl_vae_v1.0_f16.ckpt",
      "modifier": "inpainting",
      "clip_encoder": "clip_vit_l14_f16.ckpt"
    },
    {
      "name": "Fooocus Inpaint SDXL v2.6 (8-bit)",
      "file": "fooocus_inpaint_sd_xl_v2.6_q6p_q8p.ckpt",
      "prefix": "",
      "version": "sdxl_base_v0.9",
      "default_scale": 16,
      "text_encoder": "open_clip_vit_bigg14_f16.ckpt",
      "autoencoder": "sdxl_vae_v1.0_f16.ckpt",
      "modifier": "inpainting",
      "clip_encoder": "clip_vit_l14_f16.ckpt"
    },
    {
      "name": "HiDream E1 [full]",
      "file": "hidream_e1_full_q8p.ckpt",
      "prefix": "",
      "version": "hidream_i1",
      "default_scale": 12,
      "text_encoder": "llama_3.1_8b_instruct_q8p.ckpt",
      "autoencoder": "flux_1_vae_f16.ckpt",
      "modifier": "editing",
      "clip_encoder": "long_clip_vit_l14_f16.ckpt",
      "additional_clip_encoders": [
        "long_open_clip_vit_bigg14_f16.ckpt"
      ],
      "t5_encoder": "t5_xxl_encoder_q6p.ckpt",
      "high_precision_autoencoder": true,
      "padded_text_encoding_length": 128,
      "hires_fix_scale": 24,
      "note": "[HiDream-E1 [full]](https://huggingface.co/HiDream-ai/HiDream-E1-Full) is an image editing model built on HiDream-I1. It is MIT-licensed and commercially friendly. Trained at 768\u00d7768 resolution using a Flow Matching objective, the model performs best with trailing samplers and 30\u201350 sampling steps. For optimal results, ensure the width is set to 768 and use the following prompt format: Editing Instruction: {}. Target Image Description: {}."
    },
    {
      "name": "HiDream E1 [full] (5-bit)",
      "file": "hidream_e1_full_q5p.ckpt",
      "prefix": "",
      "version": "hidream_i1",
      "default_scale": 12,
      "text_encoder": "llama_3.1_8b_instruct_q8p.ckpt",
      "autoencoder": "flux_1_vae_f16.ckpt",
      "modifier": "editing",
      "clip_encoder": "long_clip_vit_l14_f16.ckpt",
      "additional_clip_encoders": [
        "long_open_clip_vit_bigg14_f16.ckpt"
      ],
      "t5_encoder": "t5_xxl_encoder_q6p.ckpt",
      "high_precision_autoencoder": true,
      "padded_text_encoding_length": 128,
      "hires_fix_scale": 24,
      "note": "[HiDream-E1 [full]](https://huggingface.co/HiDream-ai/HiDream-E1-Full) is an image editing model built on HiDream-I1. It is MIT-licensed and commercially friendly. Trained at 768\u00d7768 resolution using a Flow Matching objective, the model performs best with trailing samplers and 30\u201350 sampling steps. For optimal results, ensure the width is set to 768 and use the following prompt format: Editing Instruction: {}. Target Image Description: {}."
    },
    {
      "name": "HiDream E1-1",
      "file": "hidream_e1_1_q8p.ckpt",
      "prefix": "",
      "version": "hidream_i1",
      "default_scale": 16,
      "text_encoder": "llama_3.1_8b_instruct_q8p.ckpt",
      "autoencoder": "flux_1_vae_f16.ckpt",
      "modifier": "editing",
      "clip_encoder": "long_clip_vit_l14_f16.ckpt",
      "additional_clip_encoders": [
        "long_open_clip_vit_bigg14_f16.ckpt"
      ],
      "t5_encoder": "t5_xxl_encoder_q6p.ckpt",
      "high_precision_autoencoder": true,
      "padded_text_encoding_length": 128,
      "hires_fix_scale": 24,
      "note": "[HiDream-E1-1](https://huggingface.co/HiDream-ai/HiDream-E1-1) is an image editing model built on HiDream-I1. It is MIT-licensed and commercially friendly. Trained with dynamic resolutions (around 1MP) using a Flow Matching objective, the model performs best with trailing samplers and 30\u201350 sampling steps."
    },
    {
      "name": "HiDream E1-1 (5-bit)",
      "file": "hidream_e1_1_q5p.ckpt",
      "prefix": "",
      "version": "hidream_i1",
      "default_scale": 16,
      "text_encoder": "llama_3.1_8b_instruct_q8p.ckpt",
      "autoencoder": "flux_1_vae_f16.ckpt",
      "modifier": "editing",
      "clip_encoder": "long_clip_vit_l14_f16.ckpt",
      "additional_clip_encoders": [
        "long_open_clip_vit_bigg14_f16.ckpt"
      ],
      "t5_encoder": "t5_xxl_encoder_q6p.ckpt",
      "high_precision_autoencoder": true,
      "padded_text_encoding_length": 128,
      "hires_fix_scale": 24,
      "note": "[HiDream-E1-1](https://huggingface.co/HiDream-ai/HiDream-E1-1) is an image editing model built on HiDream-I1. It is MIT-licensed and commercially friendly. Trained with dynamic resolutions (around 1MP) using a Flow Matching objective, the model performs best with trailing samplers and 30\u201350 sampling steps."
    },
    {
      "name": "HiDream I1 [dev]",
      "file": "hidream_i1_dev_q8p.ckpt",
      "prefix": "",
      "version": "hidream_i1",
      "default_scale": 16,
      "text_encoder": "llama_3.1_8b_instruct_q8p.ckpt",
      "autoencoder": "flux_1_vae_f16.ckpt",
      "clip_encoder": "long_clip_vit_l14_f16.ckpt",
      "additional_clip_encoders": [
        "long_open_clip_vit_bigg14_f16.ckpt"
      ],
      "t5_encoder": "t5_xxl_encoder_q6p.ckpt",
      "high_precision_autoencoder": true,
      "guidance_embed": true,
      "padded_text_encoding_length": 128,
      "hires_fix_scale": 24,
      "note": "[HiDream-I1 [dev]](https://huggingface.co/HiDream-ai/HiDream-I1-Dev) is a state-of-the-art open-source image generation model known for its strong prompt adherence across diverse styles, including photorealistic, cartoon, and artistic. It is MIT-licensed and commercially friendly. The model is trained at multiple resolutions using a Flow Matching objective; trailing samplers yield the best results, with 20\u201330 sampling steps recommended. Text guidance is not effective for this model."
    },
    {
      "name": "HiDream I1 [dev] (5-bit)",
      "file": "hidream_i1_dev_q5p.ckpt",
      "prefix": "",
      "version": "hidream_i1",
      "default_scale": 16,
      "text_encoder": "llama_3.1_8b_instruct_q8p.ckpt",
      "autoencoder": "flux_1_vae_f16.ckpt",
      "clip_encoder": "long_clip_vit_l14_f16.ckpt",
      "additional_clip_encoders": [
        "long_open_clip_vit_bigg14_f16.ckpt"
      ],
      "t5_encoder": "t5_xxl_encoder_q6p.ckpt",
      "high_precision_autoencoder": true,
      "guidance_embed": true,
      "padded_text_encoding_length": 128,
      "hires_fix_scale": 24,
      "note": "[HiDream-I1 [dev]](https://huggingface.co/HiDream-ai/HiDream-I1-Dev) is a state-of-the-art open-source image generation model known for its strong prompt adherence across diverse styles, including photorealistic, cartoon, and artistic. It is MIT-licensed and commercially friendly. The model is trained at multiple resolutions using a Flow Matching objective; trailing samplers yield the best results, with 20\u201330 sampling steps recommended. Text guidance is not effective for this model."
    },
    {
      "name": "HiDream I1 [fast]",
      "file": "hidream_i1_fast_q8p.ckpt",
      "prefix": "",
      "version": "hidream_i1",
      "default_scale": 16,
      "text_encoder": "llama_3.1_8b_instruct_q8p.ckpt",
      "autoencoder": "flux_1_vae_f16.ckpt",
      "clip_encoder": "long_clip_vit_l14_f16.ckpt",
      "additional_clip_encoders": [
        "long_open_clip_vit_bigg14_f16.ckpt"
      ],
      "t5_encoder": "t5_xxl_encoder_q6p.ckpt",
      "high_precision_autoencoder": true,
      "is_consistency_model": true,
      "padded_text_encoding_length": 128,
      "hires_fix_scale": 24,
      "note": "[HiDream-I1 [fast]](https://huggingface.co/HiDream-ai/HiDream-I1-Fast) is a state-of-the-art open-source image generation model known for its strong prompt adherence across diverse styles, including photorealistic, cartoon, and artistic. It is MIT-licensed and commercially friendly. The model is trained at multiple resolutions using a Flow Matching objective; trailing samplers yield the best results, with 10\u201320 sampling steps recommended. Text guidance is not effective for this model."
    },
    {
      "name": "HiDream I1 [fast] (5-bit)",
      "file": "hidream_i1_fast_q5p.ckpt",
      "prefix": "",
      "version": "hidream_i1",
      "default_scale": 16,
      "text_encoder": "llama_3.1_8b_instruct_q8p.ckpt",
      "autoencoder": "flux_1_vae_f16.ckpt",
      "clip_encoder": "long_clip_vit_l14_f16.ckpt",
      "additional_clip_encoders": [
        "long_open_clip_vit_bigg14_f16.ckpt"
      ],
      "t5_encoder": "t5_xxl_encoder_q6p.ckpt",
      "high_precision_autoencoder": true,
      "is_consistency_model": true,
      "padded_text_encoding_length": 128,
      "hires_fix_scale": 24,
      "note": "[HiDream-I1 [fast]](https://huggingface.co/HiDream-ai/HiDream-I1-Fast) is a state-of-the-art open-source image generation model known for its strong prompt adherence across diverse styles, including photorealistic, cartoon, and artistic. It is MIT-licensed and commercially friendly. The model is trained at multiple resolutions using a Flow Matching objective; trailing samplers yield the best results, with 10\u201320 sampling steps recommended. Text guidance is not effective for this model."
    },
    {
      "name": "HiDream I1 [full]",
      "file": "hidream_i1_full_q8p.ckpt",
      "prefix": "",
      "version": "hidream_i1",
      "default_scale": 16,
      "text_encoder": "llama_3.1_8b_instruct_q8p.ckpt",
      "autoencoder": "flux_1_vae_f16.ckpt",
      "clip_encoder": "long_clip_vit_l14_f16.ckpt",
      "additional_clip_encoders": [
        "long_open_clip_vit_bigg14_f16.ckpt"
      ],
      "t5_encoder": "t5_xxl_encoder_q6p.ckpt",
      "high_precision_autoencoder": true,
      "padded_text_encoding_length": 128,
      "hires_fix_scale": 24,
      "note": "[HiDream-I1 [full]](https://huggingface.co/HiDream-ai/HiDream-I1-Full) is a state-of-the-art open-source image generation model known for its exceptional prompt adherence across a wide range of styles, including photorealistic, cartoon, and artistic. It is MIT-licensed and commercially friendly. The model is trained at multiple resolutions using a Flow Matching objective; trailing samplers yield the best results, with 30\u201350 sampling steps recommended."
    },
    {
      "name": "HiDream I1 [full] (5-bit)",
      "file": "hidream_i1_full_q5p.ckpt",
      "prefix": "",
      "version": "hidream_i1",
      "default_scale": 16,
      "text_encoder": "llama_3.1_8b_instruct_q8p.ckpt",
      "autoencoder": "flux_1_vae_f16.ckpt",
      "clip_encoder": "long_clip_vit_l14_f16.ckpt",
      "additional_clip_encoders": [
        "long_open_clip_vit_bigg14_f16.ckpt"
      ],
      "t5_encoder": "t5_xxl_encoder_q6p.ckpt",
      "high_precision_autoencoder": true,
      "padded_text_encoding_length": 128,
      "hires_fix_scale": 24,
      "note": "[HiDream-I1 [full]](https://huggingface.co/HiDream-ai/HiDream-I1-Full) is a state-of-the-art open-source image generation model known for its exceptional prompt adherence across a wide range of styles, including photorealistic, cartoon, and artistic. It is MIT-licensed and commercially friendly. The model is trained at multiple resolutions using a Flow Matching objective; trailing samplers yield the best results, with 30\u201350 sampling steps recommended."
    },
    {
      "name": "Instruct Pix2Pix",
      "file": "instruct_pix2pix_22000_f16.ckpt",
      "prefix": "",
      "version": "v1",
      "default_scale": 8,
      "modifier": "editing",
      "deprecated": true
    },
    {
      "name": "Kandinsky v2.1",
      "file": "kandinsky_f16.ckpt",
      "prefix": "",
      "version": "kandinsky2.1",
      "upcast_attention": false,
      "default_scale": 12,
      "text_encoder": "xlm_roberta_f16.ckpt",
      "autoencoder": "kandinsky_movq_f16.ckpt",
      "deprecated": true,
      "image_encoder": "image_vit_l14_f16.ckpt",
      "clip_encoder": "clip_vit_l14_f16.ckpt",
      "diffusion_mapping": "kandinsky_diffusion_mapping_f16.ckpt"
    },
    {
      "name": "MiniSD v1.4",
      "file": "minisd_v1.4_f16.ckpt",
      "prefix": "",
      "version": "v1",
      "default_scale": 4,
      "deprecated": true
    },
    {
      "name": "PixArt Sigma XL 1K",
      "file": "pixart_sigma_xl_2_1024_ms_f16.ckpt",
      "prefix": "",
      "version": "pixart",
      "default_scale": 16,
      "text_encoder": "t5_xxl_encoder_q6p.ckpt",
      "autoencoder": "sdxl_vae_v1.0_f16.ckpt"
    },
    {
      "name": "PixArt Sigma XL 1K (8-bit)",
      "file": "pixart_sigma_xl_2_1024_ms_q8p.ckpt",
      "prefix": "",
      "version": "pixart",
      "default_scale": 16,
      "text_encoder": "t5_xxl_encoder_q6p.ckpt",
      "autoencoder": "sdxl_vae_v1.0_f16.ckpt"
    },
    {
      "name": "PixArt Sigma XL 512",
      "file": "pixart_sigma_xl_2_512_ms_f16.ckpt",
      "prefix": "",
      "version": "pixart",
      "default_scale": 8,
      "text_encoder": "t5_xxl_encoder_q6p.ckpt",
      "autoencoder": "sdxl_vae_v1.0_f16.ckpt"
    },
    {
      "name": "PixArt Sigma XL 512 (8-bit)",
      "file": "pixart_sigma_xl_2_512_ms_q8p.ckpt",
      "prefix": "",
      "version": "pixart",
      "default_scale": 8,
      "text_encoder": "t5_xxl_encoder_q6p.ckpt",
      "autoencoder": "sdxl_vae_v1.0_f16.ckpt"
    },
    {
      "name": "Qwen Image 1.0",
      "file": "qwen_image_1.0_q8p.ckpt",
      "prefix": "",
      "version": "qwen_image",
      "default_scale": 16,
      "text_encoder": "qwen_2.5_vl_7b_q8p.ckpt",
      "autoencoder": "qwen_image_vae_f16.ckpt",
      "hires_fix_scale": 24,
      "note": "[Qwen Image](https://huggingface.co/Qwen/Qwen-Image) is a state-of-the-art open-source image generation model known for its exceptional text layout and prompt adherence across a wide range of styles, including photorealistic, cartoon, and artistic. It is Apache 2.0-licensed and commercially friendly. The model is trained at multiple resolutions using a Flow Matching objective; trailing samplers yield the best results, with 30\u201350 sampling steps recommended.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Qwen Image 1.0 (6-bit)",
      "file": "qwen_image_1.0_q6p.ckpt",
      "prefix": "",
      "version": "qwen_image",
      "default_scale": 16,
      "text_encoder": "qwen_2.5_vl_7b_q8p.ckpt",
      "autoencoder": "qwen_image_vae_f16.ckpt",
      "hires_fix_scale": 24,
      "note": "[Qwen Image](https://huggingface.co/Qwen/Qwen-Image) is a state-of-the-art open-source image generation model known for its exceptional text layout and prompt adherence across a wide range of styles, including photorealistic, cartoon, and artistic. It is Apache 2.0-licensed and commercially friendly. The model is trained at multiple resolutions using a Flow Matching objective; trailing samplers yield the best results, with 30\u201350 sampling steps recommended.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Qwen Image 1.0 (BF16, 6-bit)",
      "file": "qwen_image_1.0_bf16_q6p.ckpt",
      "prefix": "",
      "version": "qwen_image",
      "default_scale": 16,
      "text_encoder": "qwen_2.5_vl_7b_q8p.ckpt",
      "autoencoder": "qwen_image_vae_f16.ckpt",
      "hires_fix_scale": 24,
      "is_bf16": true,
      "note": "[Qwen Image](https://huggingface.co/Qwen/Qwen-Image) is a state-of-the-art open-source image generation model known for its exceptional text layout and prompt adherence across a wide range of styles, including photorealistic, cartoon, and artistic. It is Apache 2.0-licensed and commercially friendly. The model is trained at multiple resolutions using a Flow Matching objective; trailing samplers yield the best results, with 30\u201350 sampling steps recommended. The BF16 version is only compatible with macOS 15, iOS 18 and above.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Qwen Image 1.0 (BF16)",
      "file": "qwen_image_1.0_bf16_q8p.ckpt",
      "prefix": "",
      "version": "qwen_image",
      "default_scale": 16,
      "text_encoder": "qwen_2.5_vl_7b_q8p.ckpt",
      "autoencoder": "qwen_image_vae_f16.ckpt",
      "hires_fix_scale": 24,
      "is_bf16": true,
      "note": "[Qwen Image](https://huggingface.co/Qwen/Qwen-Image) is a state-of-the-art open-source image generation model known for its exceptional text layout and prompt adherence across a wide range of styles, including photorealistic, cartoon, and artistic. It is Apache 2.0-licensed and commercially friendly. The model is trained at multiple resolutions using a Flow Matching objective; trailing samplers yield the best results, with 30\u201350 sampling steps recommended. The BF16 version is only compatible with macOS 15, iOS 18 and above.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Qwen Image 2512 (BF16, 6-bit)",
      "file": "qwen_image_2512_bf16_q6p.ckpt",
      "prefix": "",
      "version": "qwen_image",
      "default_scale": 16,
      "text_encoder": "qwen_2.5_vl_7b_q8p.ckpt",
      "autoencoder": "qwen_image_vae_f16.ckpt",
      "hires_fix_scale": 24,
      "is_bf16": true,
      "note": "[Qwen Image 2512](https://huggingface.co/Qwen/Qwen-Image-2512) is the december update of Qwen Image model with improvements on enhanced human realism, finer natural detail and improved text rendering. It is Apache 2.0-licensed and commercially friendly. The model is trained at multiple resolutions using a Flow Matching objective; trailing samplers yield the best results, with 30\u201350 sampling steps recommended. The BF16 version is only compatible with macOS 15, iOS 18 and above.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Qwen Image 2512 (BF16)",
      "file": "qwen_image_2512_bf16_q8p.ckpt",
      "prefix": "",
      "version": "qwen_image",
      "default_scale": 16,
      "text_encoder": "qwen_2.5_vl_7b_q8p.ckpt",
      "autoencoder": "qwen_image_vae_f16.ckpt",
      "hires_fix_scale": 24,
      "is_bf16": true,
      "note": "[Qwen Image 2512](https://huggingface.co/Qwen/Qwen-Image-2512) is the december update of Qwen Image model with improvements on enhanced human realism, finer natural detail and improved text rendering. It is Apache 2.0-licensed and commercially friendly. The model is trained at multiple resolutions using a Flow Matching objective; trailing samplers yield the best results, with 30\u201350 sampling steps recommended. The BF16 version is only compatible with macOS 15, iOS 18 and above.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Qwen Image Edit 1.0",
      "file": "qwen_image_edit_1.0_q8p.ckpt",
      "prefix": "",
      "version": "qwen_image",
      "default_scale": 16,
      "text_encoder": "qwen_2.5_vl_7b_q8p.ckpt",
      "autoencoder": "qwen_image_vae_f16.ckpt",
      "modifier": "kontext",
      "clip_encoder": "qwen_2.5_vl_7b_vit_f16.ckpt",
      "hires_fix_scale": 24,
      "note": "[Qwen Image Edit](https://huggingface.co/Qwen/Qwen-Image-Edit) is a state-of-the-art open-source image edit model excels at image edit tasks such as background alternation, style transfer, object removal etc. It is Apache 2.0-licensed and commercially friendly. The model is trained at multiple resolutions using a Flow Matching objective; trailing samplers yield the best results, with 30\u201350 sampling steps recommended.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Qwen Image Edit 1.0 (6-bit)",
      "file": "qwen_image_edit_1.0_q6p.ckpt",
      "prefix": "",
      "version": "qwen_image",
      "default_scale": 16,
      "text_encoder": "qwen_2.5_vl_7b_q8p.ckpt",
      "autoencoder": "qwen_image_vae_f16.ckpt",
      "modifier": "kontext",
      "clip_encoder": "qwen_2.5_vl_7b_vit_f16.ckpt",
      "hires_fix_scale": 24,
      "note": "[Qwen Image Edit](https://huggingface.co/Qwen/Qwen-Image-Edit) is a state-of-the-art open-source image edit model excels at image edit tasks such as background alternation, style transfer, object removal etc. It is Apache 2.0-licensed and commercially friendly. The model is trained at multiple resolutions using a Flow Matching objective; trailing samplers yield the best results, with 30\u201350 sampling steps recommended.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Qwen Image Edit 2509",
      "file": "qwen_image_edit_2509_q8p.ckpt",
      "prefix": "",
      "version": "qwen_image",
      "default_scale": 16,
      "text_encoder": "qwen_2.5_vl_7b_q8p.ckpt",
      "autoencoder": "qwen_image_vae_f16.ckpt",
      "modifier": "qwenimageEditPlus",
      "clip_encoder": "qwen_2.5_vl_7b_vit_f16.ckpt",
      "hires_fix_scale": 24,
      "note": "[Qwen Image Edit 2509](https://huggingface.co/Qwen/Qwen-Image-Edit-2509) is a state-of-the-art open-source image edit model excels at image edit tasks such as background alternation, style transfer, object removal etc. It is Apache 2.0-licensed and commercially friendly. The model is trained at multiple resolutions using a Flow Matching objective; trailing samplers yield the best results, with 30\u201350 sampling steps recommended. This is an update in Sep, 2025.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Qwen Image Edit 2509 (6-bit)",
      "file": "qwen_image_edit_2509_q6p.ckpt",
      "prefix": "",
      "version": "qwen_image",
      "default_scale": 16,
      "text_encoder": "qwen_2.5_vl_7b_q8p.ckpt",
      "autoencoder": "qwen_image_vae_f16.ckpt",
      "modifier": "qwenimageEditPlus",
      "clip_encoder": "qwen_2.5_vl_7b_vit_f16.ckpt",
      "hires_fix_scale": 24,
      "note": "[Qwen Image Edit 2509](https://huggingface.co/Qwen/Qwen-Image-2509) is a state-of-the-art open-source image edit model excels at image edit tasks such as background alternation, style transfer, object removal etc. It is Apache 2.0-licensed and commercially friendly. The model is trained at multiple resolutions using a Flow Matching objective; trailing samplers yield the best results, with 30\u201350 sampling steps recommended. This is an update in Sep, 2025.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Qwen Image Edit 2509 (BF16, 6-bit)",
      "file": "qwen_image_edit_2509_bf16_q6p.ckpt",
      "prefix": "",
      "version": "qwen_image",
      "default_scale": 16,
      "text_encoder": "qwen_2.5_vl_7b_q8p.ckpt",
      "autoencoder": "qwen_image_vae_f16.ckpt",
      "modifier": "qwenimageEditPlus",
      "clip_encoder": "qwen_2.5_vl_7b_vit_f16.ckpt",
      "hires_fix_scale": 24,
      "is_bf16": true,
      "note": "[Qwen Image Edit 2509](https://huggingface.co/Qwen/Qwen-Image-2509) is a state-of-the-art open-source image edit model excels at image edit tasks such as background alternation, style transfer, object removal etc. It is Apache 2.0-licensed and commercially friendly. The model is trained at multiple resolutions using a Flow Matching objective; trailing samplers yield the best results, with 30\u201350 sampling steps recommended. This is an update in Sep, 2025. The BF16 version is only compatible with macOS 15, iOS 18 and above.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Qwen Image Edit 2509 (BF16)",
      "file": "qwen_image_edit_2509_bf16_q8p.ckpt",
      "prefix": "",
      "version": "qwen_image",
      "default_scale": 16,
      "text_encoder": "qwen_2.5_vl_7b_q8p.ckpt",
      "autoencoder": "qwen_image_vae_f16.ckpt",
      "modifier": "qwenimageEditPlus",
      "clip_encoder": "qwen_2.5_vl_7b_vit_f16.ckpt",
      "hires_fix_scale": 24,
      "is_bf16": true,
      "note": "[Qwen Image Edit 2509](https://huggingface.co/Qwen/Qwen-Image-Edit-2509) is a state-of-the-art open-source image edit model excels at image edit tasks such as background alternation, style transfer, object removal etc. It is Apache 2.0-licensed and commercially friendly. The model is trained at multiple resolutions using a Flow Matching objective; trailing samplers yield the best results, with 30\u201350 sampling steps recommended. This is an update in Sep, 2025. The BF16 version is only compatible with macOS 15, iOS 18 and above.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Qwen Image Edit 2511",
      "file": "qwen_image_edit_2511_q8p.ckpt",
      "prefix": "",
      "version": "qwen_image",
      "default_scale": 16,
      "text_encoder": "qwen_2.5_vl_7b_q8p.ckpt",
      "autoencoder": "qwen_image_vae_f16.ckpt",
      "modifier": "qwenimageEdit2511",
      "clip_encoder": "qwen_2.5_vl_7b_vit_f16.ckpt",
      "hires_fix_scale": 24,
      "note": "[Qwen Image Edit 2511](https://huggingface.co/Qwen/Qwen-Image-Edit-2511) is an enhanced image editing model that significantly improves character consistency, mitigates image drift, and strengthens multi-person fusion capabilities compared to its predecessor (2509). It integrates popular LoRA features natively, enabling advanced lighting control and viewpoint generation without extra tuning, alongside specialized industrial design and geometric reasoning capabilities. It is Apache 2.0-licensed, with 40 inference steps recommended for optimal results.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Qwen Image Edit 2511 (6-bit)",
      "file": "qwen_image_edit_2511_q6p.ckpt",
      "prefix": "",
      "version": "qwen_image",
      "default_scale": 16,
      "text_encoder": "qwen_2.5_vl_7b_q8p.ckpt",
      "autoencoder": "qwen_image_vae_f16.ckpt",
      "modifier": "qwenimageEdit2511",
      "clip_encoder": "qwen_2.5_vl_7b_vit_f16.ckpt",
      "hires_fix_scale": 24,
      "note": "[Qwen Image Edit 2511](https://huggingface.co/Qwen/Qwen-Image-Edit-2511) is an enhanced image editing model that significantly improves character consistency, mitigates image drift, and strengthens multi-person fusion capabilities compared to its predecessor (2509). It integrates popular LoRA features natively, enabling advanced lighting control and viewpoint generation without extra tuning, alongside specialized industrial design and geometric reasoning capabilities. It is Apache 2.0-licensed, with 40 inference steps recommended for optimal results.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Qwen Image Edit 2511 (BF16, 6-bit)",
      "file": "qwen_image_edit_2511_bf16_q6p.ckpt",
      "prefix": "",
      "version": "qwen_image",
      "default_scale": 16,
      "text_encoder": "qwen_2.5_vl_7b_q8p.ckpt",
      "autoencoder": "qwen_image_vae_f16.ckpt",
      "modifier": "qwenimageEdit2511",
      "clip_encoder": "qwen_2.5_vl_7b_vit_f16.ckpt",
      "hires_fix_scale": 24,
      "is_bf16": true,
      "note": "[Qwen Image Edit 2511](https://huggingface.co/Qwen/Qwen-Image-Edit-2511) is an enhanced image editing model that significantly improves character consistency, mitigates image drift, and strengthens multi-person fusion capabilities compared to its predecessor (2509). It integrates popular LoRA features natively, enabling advanced lighting control and viewpoint generation without extra tuning, alongside specialized industrial design and geometric reasoning capabilities. It is Apache 2.0-licensed, with 40 inference steps recommended for optimal results. The BF16 version is only compatible with macOS 15, iOS 18 and above.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Qwen Image Edit 2511 (BF16)",
      "file": "qwen_image_edit_2511_bf16_q8p.ckpt",
      "prefix": "",
      "version": "qwen_image",
      "default_scale": 16,
      "text_encoder": "qwen_2.5_vl_7b_q8p.ckpt",
      "autoencoder": "qwen_image_vae_f16.ckpt",
      "modifier": "qwenimageEdit2511",
      "clip_encoder": "qwen_2.5_vl_7b_vit_f16.ckpt",
      "hires_fix_scale": 24,
      "is_bf16": true,
      "note": "[Qwen Image Edit 2511](https://huggingface.co/Qwen/Qwen-Image-Edit-2511) is an enhanced image editing model that significantly improves character consistency, mitigates image drift, and strengthens multi-person fusion capabilities compared to its predecessor (2509). It integrates popular LoRA features natively, enabling advanced lighting control and viewpoint generation without extra tuning, alongside specialized industrial design and geometric reasoning capabilities. It is Apache 2.0-licensed, with 40 inference steps recommended for optimal results. The BF16 version is only compatible with macOS 15, iOS 18 and above.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Qwen Image Layered 1.0 (BF16, 6-bit)",
      "file": "qwen_image_layered_1.0_bf16_q6p.ckpt",
      "prefix": "",
      "version": "qwen_image",
      "default_scale": 16,
      "text_encoder": "qwen_2.5_vl_7b_q8p.ckpt",
      "autoencoder": "qwen_image_layered_vae_f16.ckpt",
      "modifier": "qwenimageLayered",
      "hires_fix_scale": 24,
      "is_bf16": true,
      "note": "[Qwen Image Layered](https://huggingface.co/Qwen/Qwen-Image-Layered) is a specialized model capable of decomposing an image into multiple transparent RGBA layers to unlock inherent editability. By physically isolating semantic components, it enables high-fidelity operations such as resizing, repositioning, and recoloring without affecting the rest of the image. It is Apache 2.0-licensed and commercially friendly. The model supports flexible and recursive decomposition, allowing users to define specific layer counts (Batch Size), with a recommended resolution of 640px and 50 inference steps. The BF16 version is only compatible with macOS 15, iOS 18 and above.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Qwen Image Layered 1.0 (BF16)",
      "file": "qwen_image_layered_1.0_bf16_q8p.ckpt",
      "prefix": "",
      "version": "qwen_image",
      "default_scale": 16,
      "text_encoder": "qwen_2.5_vl_7b_q8p.ckpt",
      "autoencoder": "qwen_image_layered_vae_f16.ckpt",
      "modifier": "qwenimageLayered",
      "hires_fix_scale": 24,
      "is_bf16": true,
      "note": "[Qwen Image Layered](https://huggingface.co/Qwen/Qwen-Image-Layered) is a specialized model capable of decomposing an image into multiple transparent RGBA layers to unlock inherent editability. By physically isolating semantic components, it enables high-fidelity operations such as resizing, repositioning, and recoloring without affecting the rest of the image. It is Apache 2.0-licensed and commercially friendly. The model supports flexible and recursive decomposition, allowing users to define specific layer counts (Batch Size), with a recommended resolution of 640px and 50 inference steps. The BF16 version is only compatible with macOS 15, iOS 18 and above.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "SDXL Base (v0.9)",
      "file": "sd_xl_base_0.9_f16.ckpt",
      "prefix": "",
      "version": "sdxl_base_v0.9",
      "default_scale": 16,
      "text_encoder": "open_clip_vit_bigg14_f16.ckpt",
      "autoencoder": "sdxl_vae_f16.ckpt",
      "deprecated": true,
      "clip_encoder": "clip_vit_l14_f16.ckpt"
    },
    {
      "name": "SDXL Base (v1.0)",
      "file": "sd_xl_base_1.0_f16.ckpt",
      "prefix": "",
      "version": "sdxl_base_v0.9",
      "default_scale": 16,
      "text_encoder": "open_clip_vit_bigg14_f16.ckpt",
      "autoencoder": "sdxl_vae_v1.0_f16.ckpt",
      "clip_encoder": "clip_vit_l14_f16.ckpt"
    },
    {
      "name": "SDXL Base v1.0 (8-bit)",
      "file": "sd_xl_base_1.0_q6p_q8p.ckpt",
      "prefix": "",
      "version": "sdxl_base_v0.9",
      "default_scale": 16,
      "text_encoder": "open_clip_vit_bigg14_f16.ckpt",
      "autoencoder": "sdxl_vae_v1.0_f16.ckpt",
      "clip_encoder": "clip_vit_l14_f16.ckpt"
    },
    {
      "name": "SDXL Refiner (v0.9)",
      "file": "sd_xl_refiner_0.9_f16.ckpt",
      "prefix": "",
      "version": "sdxl_refiner_v0.9",
      "default_scale": 16,
      "text_encoder": "open_clip_vit_bigg14_f16.ckpt",
      "autoencoder": "sdxl_vae_f16.ckpt",
      "deprecated": true,
      "clip_encoder": "clip_vit_l14_f16.ckpt"
    },
    {
      "name": "SDXL Refiner (v1.0)",
      "file": "sd_xl_refiner_1.0_f16.ckpt",
      "prefix": "",
      "version": "sdxl_refiner_v0.9",
      "default_scale": 16,
      "text_encoder": "open_clip_vit_bigg14_f16.ckpt",
      "autoencoder": "sdxl_vae_v1.0_f16.ckpt",
      "clip_encoder": "clip_vit_l14_f16.ckpt"
    },
    {
      "name": "SDXL Refiner v1.0 (8-bit)",
      "file": "sd_xl_refiner_1.0_q6p_q8p.ckpt",
      "prefix": "",
      "version": "sdxl_refiner_v0.9",
      "default_scale": 16,
      "text_encoder": "open_clip_vit_bigg14_f16.ckpt",
      "autoencoder": "sdxl_vae_v1.0_f16.ckpt",
      "clip_encoder": "clip_vit_l14_f16.ckpt"
    },
    {
      "name": "Stable Diffusion v1.4",
      "file": "sd_v1.4_f16.ckpt",
      "prefix": "",
      "version": "v1",
      "deprecated": true
    },
    {
      "name": "Stable Diffusion v1.5",
      "file": "sd_v1.5_f16.ckpt",
      "prefix": "",
      "version": "v1"
    },
    {
      "name": "Stable Diffusion v1.5 Inpainting",
      "file": "sd_v1.5_inpainting_f16.ckpt",
      "prefix": "",
      "version": "v1",
      "modifier": "inpainting"
    },
    {
      "name": "Stable Diffusion v2.0",
      "file": "sd_v2.0_f16.ckpt",
      "prefix": "",
      "version": "v2",
      "text_encoder": "open_clip_vit_h14_f16.ckpt",
      "deprecated": true
    },
    {
      "name": "Stable Diffusion v2.0 768-v",
      "file": "sd_v2.0_768_v_f16.ckpt",
      "prefix": "",
      "version": "v2",
      "default_scale": 12,
      "text_encoder": "open_clip_vit_h14_f16.ckpt",
      "deprecated": true
    },
    {
      "name": "Stable Diffusion v2.0 Depth",
      "file": "sd_v2.0_depth_f16.ckpt",
      "prefix": "",
      "version": "v2",
      "text_encoder": "open_clip_vit_h14_f16.ckpt",
      "modifier": "depth"
    },
    {
      "name": "Stable Diffusion v2.0 Inpainting",
      "file": "sd_v2.0_inpainting_f16.ckpt",
      "prefix": "",
      "version": "v2",
      "text_encoder": "open_clip_vit_h14_f16.ckpt",
      "modifier": "inpainting"
    },
    {
      "name": "Stable Diffusion v2.1",
      "file": "sd_v2.1_f16.ckpt",
      "prefix": "",
      "version": "v2",
      "text_encoder": "open_clip_vit_h14_f16.ckpt"
    },
    {
      "name": "Stable Diffusion v2.1 768-v",
      "file": "sd_v2.1_768_v_f16.ckpt",
      "prefix": "",
      "version": "v2",
      "upcast_attention": true,
      "default_scale": 12,
      "text_encoder": "open_clip_vit_h14_f16.ckpt"
    },
    {
      "name": "Stable Video Diffusion I2V XT 1.0 (8-bit)",
      "file": "svd_i2v_xt_1.0_q6p_q8p.ckpt",
      "prefix": "",
      "version": "svd_i2v",
      "default_scale": 8,
      "text_encoder": "open_clip_vit_h14_vision_model_f16.ckpt",
      "deprecated": true,
      "clip_encoder": "svd_i2v_xt_1.0_q6p_q8p.ckpt"
    },
    {
      "name": "Stable Video Diffusion I2V XT v1.0",
      "file": "svd_i2v_xt_1.0_f16.ckpt",
      "prefix": "",
      "version": "svd_i2v",
      "default_scale": 8,
      "text_encoder": "open_clip_vit_h14_vision_model_f16.ckpt",
      "deprecated": true,
      "clip_encoder": "svd_i2v_xt_1.0_f16.ckpt"
    },
    {
      "name": "Wan 2.1 I2V 14B 480p",
      "file": "wan_v2.1_14b_i2v_480p_q8p.ckpt",
      "prefix": "",
      "version": "wan_v2.1_14b",
      "default_scale": 8,
      "text_encoder": "umt5_xxl_encoder_q8p.ckpt",
      "autoencoder": "wan_v2.1_video_vae_f16.ckpt",
      "modifier": "inpainting",
      "clip_encoder": "open_clip_xlm_roberta_large_vit_h14_f16.ckpt",
      "hires_fix_scale": 12,
      "tea_cache_coefficients": [
        257151.496,
        -35422.9917,
        1402.86849,
        -13.5890334,
        0.132517977
      ],
      "frames_per_second": 16,
      "note": "[Wan2.1 I2V 14B 480P](https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-480P) is a state-of-the-art image-to-video model developed by Alibaba. It can generate video clips of up to 4 seconds in length from a given start frame. The recommended resolutions are 832\u00d7480. The model supports up to 81 frames, with a recommended shift value of 5.0.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Wan 2.1 I2V 14B 480p (6-bit, SVDQuant)",
      "file": "wan_v2.1_14b_i2v_480p_q6p_svd.ckpt",
      "prefix": "",
      "version": "wan_v2.1_14b",
      "default_scale": 8,
      "text_encoder": "umt5_xxl_encoder_q8p.ckpt",
      "autoencoder": "wan_v2.1_video_vae_f16.ckpt",
      "modifier": "inpainting",
      "clip_encoder": "open_clip_xlm_roberta_large_vit_h14_f16.ckpt",
      "hires_fix_scale": 12,
      "builtin_lora": true,
      "tea_cache_coefficients": [
        257151.496,
        -35422.9917,
        1402.86849,
        -13.5890334,
        0.132517977
      ],
      "frames_per_second": 16,
      "note": "[Wan2.1 I2V 14B 480P](https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-480P) is a state-of-the-art image-to-video model developed by Alibaba. It can generate video clips of up to 4 seconds in length from a given start frame. The recommended resolutions are 832\u00d7480. The model supports up to 81 frames, with a recommended shift value of 5.0.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Wan 2.1 I2V 14B 720p",
      "file": "wan_v2.1_14b_i2v_720p_q8p.ckpt",
      "prefix": "",
      "version": "wan_v2.1_14b",
      "default_scale": 12,
      "text_encoder": "umt5_xxl_encoder_q8p.ckpt",
      "autoencoder": "wan_v2.1_video_vae_f16.ckpt",
      "modifier": "inpainting",
      "clip_encoder": "open_clip_xlm_roberta_large_vit_h14_f16.ckpt",
      "hires_fix_scale": 16,
      "tea_cache_coefficients": [
        8107.0546,
        2133.93892,
        -372.934672,
        16.6203073,
        -0.0417769401
      ],
      "frames_per_second": 16,
      "note": "[Wan2.1 I2V 14B 720P](https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-720P) is a state-of-the-art image-to-video model developed by Alibaba. It can generate video clips of up to 4 seconds in length from a given start frame. The recommended resolutions are 1280\u00d7720. The model supports up to 81 frames, with a recommended shift value of 5.0.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Wan 2.1 I2V 14B 720p (6-bit, SVDQuant)",
      "file": "wan_v2.1_14b_i2v_720p_q6p_svd.ckpt",
      "prefix": "",
      "version": "wan_v2.1_14b",
      "default_scale": 12,
      "text_encoder": "umt5_xxl_encoder_q8p.ckpt",
      "autoencoder": "wan_v2.1_video_vae_f16.ckpt",
      "modifier": "inpainting",
      "clip_encoder": "open_clip_xlm_roberta_large_vit_h14_f16.ckpt",
      "hires_fix_scale": 16,
      "builtin_lora": true,
      "tea_cache_coefficients": [
        8107.0546,
        2133.93892,
        -372.934672,
        16.6203073,
        -0.0417769401
      ],
      "frames_per_second": 16,
      "note": "[Wan2.1 I2V 14B 720P](https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-720P) is a state-of-the-art image-to-video model developed by Alibaba. It can generate video clips of up to 4 seconds in length from a given start frame. The recommended resolutions are 1280\u00d7720. The model supports up to 81 frames, with a recommended shift value of 5.0.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Wan 2.1 T2V 1.3B",
      "file": "wan_v2.1_1.3b_480p_f16.ckpt",
      "prefix": "",
      "version": "wan_v2.1_1.3b",
      "default_scale": 8,
      "text_encoder": "umt5_xxl_encoder_q8p.ckpt",
      "autoencoder": "wan_v2.1_video_vae_f16.ckpt",
      "hires_fix_scale": 12,
      "tea_cache_coefficients": [
        -52186.2437,
        9230.41404,
        -528.275948,
        13.6987616,
        -0.0499875664
      ],
      "frames_per_second": 16,
      "note": "[Wan2.1 T2V 1.3B](https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B) is a state-of-the-art text-to-video model developed by Alibaba. It can generate video clips of up to 4 seconds in length. The recommended resolutions are 832\u00d7480. The model supports up to 81 frames, with a recommended shift value of 6.0. For best results, set Text Guidance above 5.0. Wan2.1 is trained with a Flow Matching objective, and trailing samplers will produce the best outputs.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Wan 2.1 T2V 1.3B (8-bit)",
      "file": "wan_v2.1_1.3b_480p_q8p.ckpt",
      "prefix": "",
      "version": "wan_v2.1_1.3b",
      "default_scale": 8,
      "text_encoder": "umt5_xxl_encoder_q8p.ckpt",
      "autoencoder": "wan_v2.1_video_vae_f16.ckpt",
      "hires_fix_scale": 12,
      "tea_cache_coefficients": [
        -52186.2437,
        9230.41404,
        -528.275948,
        13.6987616,
        -0.0499875664
      ],
      "frames_per_second": 16,
      "note": "[Wan2.1 T2V 1.3B](https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B) is a state-of-the-art text-to-video model developed by Alibaba. It can generate video clips of up to 4 seconds in length. The recommended resolutions are 832\u00d7480. The model supports up to 81 frames, with a recommended shift value of 6.0. For best results, set Text Guidance above 5.0. Wan2.1 is trained with a Flow Matching objective, and trailing samplers will produce the best outputs.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Wan 2.1 T2V 14B",
      "file": "wan_v2.1_14b_720p_q8p.ckpt",
      "prefix": "",
      "version": "wan_v2.1_14b",
      "default_scale": 12,
      "text_encoder": "umt5_xxl_encoder_q8p.ckpt",
      "autoencoder": "wan_v2.1_video_vae_f16.ckpt",
      "hires_fix_scale": 16,
      "tea_cache_coefficients": [
        -303318.725,
        49053.7029,
        -2655.30556,
        58.7365115,
        -0.315583525
      ],
      "frames_per_second": 16,
      "note": "[Wan2.1 T2V 14B](https://huggingface.co/Wan-AI/Wan2.1-T2V-14B) is a state-of-the-art text-to-video model developed by Alibaba. It can generate video clips of up to 4 seconds in length. The recommended resolutions are 1280\u00d7720. The recommended resolutions are 832\u00d7480. The model supports up to 81 frames, with a recommended shift value of 5.0. For best results, set Text Guidance above 5.0. Wan2.1 is trained with a Flow Matching objective, and trailing samplers will produce the best outputs.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Wan 2.1 T2V 14B (5-bit, SVDQuant)",
      "file": "wan_v2.1_14b_720p_q5p_svd.ckpt",
      "prefix": "",
      "version": "wan_v2.1_14b",
      "default_scale": 12,
      "text_encoder": "umt5_xxl_encoder_q8p.ckpt",
      "autoencoder": "wan_v2.1_video_vae_f16.ckpt",
      "deprecated": true,
      "hires_fix_scale": 16,
      "builtin_lora": true,
      "tea_cache_coefficients": [
        -303318.725,
        49053.7029,
        -2655.30556,
        58.7365115,
        -0.315583525
      ],
      "frames_per_second": 16,
      "note": "[Wan2.1 T2V 14B](https://huggingface.co/Wan-AI/Wan2.1-T2V-14B) is a state-of-the-art text-to-video model developed by Alibaba. It can generate video clips of up to 4 seconds in length. The recommended resolutions are 1280\u00d7720. The model supports up to 81 frames, with a recommended shift value of 5.0.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Wan 2.1 T2V 14B (6-bit, SVDQuant)",
      "file": "wan_v2.1_14b_720p_q6p_svd.ckpt",
      "prefix": "",
      "version": "wan_v2.1_14b",
      "default_scale": 12,
      "text_encoder": "umt5_xxl_encoder_q8p.ckpt",
      "autoencoder": "wan_v2.1_video_vae_f16.ckpt",
      "hires_fix_scale": 16,
      "builtin_lora": true,
      "tea_cache_coefficients": [
        -303318.725,
        49053.7029,
        -2655.30556,
        58.7365115,
        -0.315583525
      ],
      "frames_per_second": 16,
      "note": "[Wan2.1 T2V 14B](https://huggingface.co/Wan-AI/Wan2.1-T2V-14B) is a state-of-the-art text-to-video model developed by Alibaba. It can generate video clips of up to 4 seconds in length. The recommended resolutions are 1280\u00d7720. The model supports up to 81 frames, with a recommended shift value of 5.0.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Wan 2.2 High Noise Expert I2V A14B",
      "file": "wan_v2.2_a14b_hne_i2v_q8p.ckpt",
      "prefix": "",
      "version": "wan_v2.1_14b",
      "default_scale": 12,
      "text_encoder": "umt5_xxl_encoder_q8p.ckpt",
      "autoencoder": "wan_v2.1_video_vae_f16.ckpt",
      "modifier": "inpainting",
      "hires_fix_scale": 16,
      "tea_cache_coefficients": [
        257151.496,
        -35422.9917,
        1402.86849,
        -13.5890334,
        0.132517977
      ],
      "frames_per_second": 16,
      "note": "[Wan2.2 I2V A14B](https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B) is a state-of-the-art image-to-video model developed by Alibaba. It can generate video clips of up to 4 seconds in length from a given start frame. The recommended resolutions are 1280\u00d7720. The model supports up to 81 frames, with a recommended shift value of 5.0.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Wan 2.2 High Noise Expert I2V A14B (6-bit, SVDQuant)",
      "file": "wan_v2.2_a14b_hne_i2v_q6p_svd.ckpt",
      "prefix": "",
      "version": "wan_v2.1_14b",
      "default_scale": 12,
      "text_encoder": "umt5_xxl_encoder_q8p.ckpt",
      "autoencoder": "wan_v2.1_video_vae_f16.ckpt",
      "modifier": "inpainting",
      "hires_fix_scale": 16,
      "builtin_lora": true,
      "tea_cache_coefficients": [
        257151.496,
        -35422.9917,
        1402.86849,
        -13.5890334,
        0.132517977
      ],
      "frames_per_second": 16,
      "note": "[Wan2.2 I2V A14B](https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B) is a state-of-the-art image-to-video model developed by Alibaba. It can generate video clips of up to 4 seconds in length from a given start frame. The recommended resolutions are 1280\u00d7720. The model supports up to 81 frames, with a recommended shift value of 5.0.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Wan 2.2 High Noise Expert T2V A14B",
      "file": "wan_v2.2_a14b_hne_t2v_q8p.ckpt",
      "prefix": "",
      "version": "wan_v2.1_14b",
      "default_scale": 12,
      "text_encoder": "umt5_xxl_encoder_q8p.ckpt",
      "autoencoder": "wan_v2.1_video_vae_f16.ckpt",
      "hires_fix_scale": 16,
      "tea_cache_coefficients": [
        -303318.725,
        49053.7029,
        -2655.30556,
        58.7365115,
        -0.315583525
      ],
      "frames_per_second": 16,
      "note": "[Wan2.2 T2V A14B](https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B) is a state-of-the-art text-to-video model developed by Alibaba. It can generate video clips of up to 4 seconds in length. The recommended resolutions are 1280\u00d7720. The model supports up to 81 frames, with a recommended shift value of 5.0.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Wan 2.2 High Noise Expert T2V A14B (6-bit, SVDQuant)",
      "file": "wan_v2.2_a14b_hne_t2v_q6p_svd.ckpt",
      "prefix": "",
      "version": "wan_v2.1_14b",
      "default_scale": 12,
      "text_encoder": "umt5_xxl_encoder_q8p.ckpt",
      "autoencoder": "wan_v2.1_video_vae_f16.ckpt",
      "hires_fix_scale": 16,
      "builtin_lora": true,
      "tea_cache_coefficients": [
        -303318.725,
        49053.7029,
        -2655.30556,
        58.7365115,
        -0.315583525
      ],
      "frames_per_second": 16,
      "note": "[Wan2.2 T2V A14B](https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B) is a state-of-the-art text-to-video model developed by Alibaba. It can generate video clips of up to 4 seconds in length. The recommended resolutions are 1280\u00d7720. The model supports up to 81 frames, with a recommended shift value of 5.0.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Wan 2.2 Low Noise Expert I2V A14B",
      "file": "wan_v2.2_a14b_lne_i2v_q8p.ckpt",
      "prefix": "",
      "version": "wan_v2.1_14b",
      "default_scale": 12,
      "text_encoder": "umt5_xxl_encoder_q8p.ckpt",
      "autoencoder": "wan_v2.1_video_vae_f16.ckpt",
      "modifier": "inpainting",
      "hires_fix_scale": 16,
      "tea_cache_coefficients": [
        257151.496,
        -35422.9917,
        1402.86849,
        -13.5890334,
        0.132517977
      ],
      "frames_per_second": 16,
      "note": "[Wan2.2 I2V A14B](https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B) is a state-of-the-art image-to-video model developed by Alibaba. It can generate video clips of up to 4 seconds in length from a given start frame. The recommended resolutions are 1280\u00d7720. The model supports up to 81 frames, with a recommended shift value of 5.0.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Wan 2.2 Low Noise Expert I2V A14B (6-bit, SVDQuant)",
      "file": "wan_v2.2_a14b_lne_i2v_q6p_svd.ckpt",
      "prefix": "",
      "version": "wan_v2.1_14b",
      "default_scale": 12,
      "text_encoder": "umt5_xxl_encoder_q8p.ckpt",
      "autoencoder": "wan_v2.1_video_vae_f16.ckpt",
      "modifier": "inpainting",
      "hires_fix_scale": 16,
      "builtin_lora": true,
      "tea_cache_coefficients": [
        257151.496,
        -35422.9917,
        1402.86849,
        -13.5890334,
        0.132517977
      ],
      "frames_per_second": 16,
      "note": "[Wan2.2 I2V A14B](https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B) is a state-of-the-art image-to-video model developed by Alibaba. It can generate video clips of up to 4 seconds in length from a given start frame. The recommended resolutions are 1280\u00d7720. The model supports up to 81 frames, with a recommended shift value of 5.0.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Wan 2.2 Low Noise Expert T2V A14B",
      "file": "wan_v2.2_a14b_lne_t2v_q8p.ckpt",
      "prefix": "",
      "version": "wan_v2.1_14b",
      "default_scale": 12,
      "text_encoder": "umt5_xxl_encoder_q8p.ckpt",
      "autoencoder": "wan_v2.1_video_vae_f16.ckpt",
      "hires_fix_scale": 16,
      "tea_cache_coefficients": [
        -303318.725,
        49053.7029,
        -2655.30556,
        58.7365115,
        -0.315583525
      ],
      "frames_per_second": 16,
      "note": "[Wan2.2 T2V A14B](https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B) is a state-of-the-art text-to-video model developed by Alibaba. It can generate video clips of up to 4 seconds in length. The recommended resolutions are 1280\u00d7720. The model supports up to 81 frames, with a recommended shift value of 5.0.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Wan 2.2 Low Noise Expert T2V A14B (6-bit, SVDQuant)",
      "file": "wan_v2.2_a14b_lne_t2v_q6p_svd.ckpt",
      "prefix": "",
      "version": "wan_v2.1_14b",
      "default_scale": 12,
      "text_encoder": "umt5_xxl_encoder_q8p.ckpt",
      "autoencoder": "wan_v2.1_video_vae_f16.ckpt",
      "hires_fix_scale": 16,
      "builtin_lora": true,
      "tea_cache_coefficients": [
        -303318.725,
        49053.7029,
        -2655.30556,
        58.7365115,
        -0.315583525
      ],
      "frames_per_second": 16,
      "note": "[Wan2.2 T2V A14B](https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B) is a state-of-the-art text-to-video model developed by Alibaba. It can generate video clips of up to 4 seconds in length. The recommended resolutions are 1280\u00d7720. The model supports up to 81 frames, with a recommended shift value of 5.0.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Wan 2.2 TI2V 5B",
      "file": "wan_v2.2_5b_ti2v_f16.ckpt",
      "prefix": "",
      "version": "wan_v2.2_5b",
      "default_scale": 12,
      "text_encoder": "umt5_xxl_encoder_q8p.ckpt",
      "autoencoder": "wan_v2.2_video_vae_f16.ckpt",
      "hires_fix_scale": 16,
      "tea_cache_coefficients": null,
      "frames_per_second": 24,
      "note": "[Wan2.2 TI2V 5B](https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B) is a state-of-the-art text-image-to-video model developed by Alibaba. It can generate video clips of up to 4 seconds in length. The recommended resolutions are 1280\u00d7720. The model supports up to 121 frames, with a recommended shift value of 5.0.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Wan 2.2 TI2V 5B (8-bit)",
      "file": "wan_v2.2_5b_ti2v_q8p.ckpt",
      "prefix": "",
      "version": "wan_v2.2_5b",
      "default_scale": 12,
      "text_encoder": "umt5_xxl_encoder_q8p.ckpt",
      "autoencoder": "wan_v2.2_video_vae_f16.ckpt",
      "hires_fix_scale": 16,
      "tea_cache_coefficients": null,
      "frames_per_second": 24,
      "note": "[Wan2.2 TI2V 5B](https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B) is a state-of-the-art text-image-to-video model developed by Alibaba. It can generate video clips of up to 4 seconds in length. The recommended resolutions are 1280\u00d7720. The model supports up to 121 frames, with a recommended shift value of 5.0.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Z Image Turbo 1.0",
      "file": "z_image_turbo_1.0_q8p.ckpt",
      "prefix": "",
      "version": "zImage",
      "default_scale": 16,
      "text_encoder": "qwen_3_vl_4b_instruct_q8p.ckpt",
      "autoencoder": "flux_1_vae_f16.ckpt",
      "hires_fix_scale": 24,
      "note": "[Z Image Turbo](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo) is is a powerful and highly efficient image generation model with 6B parameters. It is Apache 2.0-licensed and commercially friendly. The model is trained at multiple resolutions using a Flow Matching objective; trailing samplers yield the best results, with 8 sampling steps recommended.",
      "copyright": "\u00a9 2025 Alibaba"
    },
    {
      "name": "Z Image Turbo 1.0 (6-bit)",
      "file": "z_image_turbo_1.0_q6p.ckpt",
      "prefix": "",
      "version": "zImage",
      "default_scale": 16,
      "text_encoder": "qwen_3_vl_4b_instruct_q8p.ckpt",
      "autoencoder": "flux_1_vae_f16.ckpt",
      "hires_fix_scale": 24,
      "note": "[Z Image Turbo](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo) is is a powerful and highly efficient image generation model with 6B parameters. It is Apache 2.0-licensed and commercially friendly. The model is trained at multiple resolutions using a Flow Matching objective; trailing samplers yield the best results, with 8 sampling steps recommended.",
      "copyright": "\u00a9 2025 Alibaba"
    }
  ],
  "loras": [
    {
      "name": "Fooocus Inpaint v2.6",
      "file": "fooocus_inpaint_v2.6_lora_f16.ckpt",
      "prefix": "",
      "version": "sdxl_base_v0.9",
      "modifier": "inpainting"
    },
    {
      "name": "Fooocus Inpaint v2.6 (8-bit)",
      "file": "fooocus_inpaint_v2.6_lora_q8p.ckpt",
      "prefix": "",
      "version": "sdxl_base_v0.9",
      "modifier": "inpainting"
    },
    {
      "name": "Foreground to Blending",
      "file": "layer_xl_fg2ble_v1.0_lora_f16.ckpt",
      "prefix": "",
      "version": "sdxl_base_v0.9",
      "modifier": "inpainting"
    },
    {
      "name": "LCM SDXL Base (1.0)",
      "file": "lcm_sd_xl_base_1.0_lora_f16.ckpt",
      "prefix": "",
      "version": "sdxl_base_v0.9",
      "is_consistency_model": true
    },
    {
      "name": "LCM SDXL Refiner (1.0)",
      "file": "lcm_sd_xl_refiner_1.0_lora_f16.ckpt",
      "prefix": "",
      "version": "sdxl_refiner_v0.9",
      "is_consistency_model": true
    },
    {
      "name": "LCM Stable Diffusion v1.5",
      "file": "lcm_sd_v1.5_lora_f16.ckpt",
      "prefix": "",
      "version": "v1",
      "is_consistency_model": true
    },
    {
      "name": "SDXL Offset (1.0)",
      "file": "sdxl_offset_v1.0_lora_f16.ckpt",
      "prefix": "",
      "version": "sdxl_base_v0.9"
    },
    {
      "name": "TCD SDXL Base (1.0)",
      "file": "tcd_sd_xl_base_1.0_lora_f16.ckpt",
      "prefix": "",
      "version": "sdxl_base_v0.9",
      "is_consistency_model": true
    },
    {
      "name": "TCD Stable Diffusion v1.5",
      "file": "tcd_sd_v1.5_lora_f16.ckpt",
      "prefix": "",
      "version": "v1",
      "is_consistency_model": true
    },
    {
      "name": "Transparent Image",
      "file": "layer_flux_1_transparent_lora_f16.ckpt",
      "prefix": "",
      "version": "flux1",
      "alternative_decoder": "flux_1_transparent_vae_decoder_f16.ckpt",
      "alternative_decoder_version": "transparent"
    },
    {
      "name": "Transparent Image (Attention Injection)",
      "file": "layer_xl_transparent_attn_v1.0_lora_f16.ckpt",
      "prefix": "",
      "version": "sdxl_base_v0.9",
      "alternative_decoder": "transparent_vae_decoder_v1.0_f16.ckpt",
      "alternative_decoder_version": "transparent"
    }
  ],
  "controlNets": [
    {
      "name": "Canny Edge Map (SD v1.x, ControlNet 1.0)",
      "file": "controlnet_canny_1.x_f16.ckpt",
      "modifier": "canny",
      "version": "v1",
      "type": "controlnet",
      "deprecated": true
    },
    {
      "name": "Canny Edge Map (SD v1.x, ControlNet 1.1)",
      "file": "controlnet_canny_1.x_v1.1_f16.ckpt",
      "modifier": "canny",
      "version": "v1",
      "type": "controlnet"
    },
    {
      "name": "Canny Edge Map (SD v1.x, T2I Adapter)",
      "file": "t2iadapter_canny_1.x_f16.ckpt",
      "modifier": "canny",
      "version": "v1",
      "type": "t2iadapter"
    },
    {
      "name": "Canny Edge Map (SD v2.x, ControlNet)",
      "file": "controlnet_canny_2.x_f16.ckpt",
      "modifier": "canny",
      "version": "v2",
      "type": "controlnet"
    },
    {
      "name": "Canny Edge Map (SDXL, ControlNet, Diffusers 1.0 Mid)",
      "file": "controlnet_canny_sdxl_v1.0_mid_f16.ckpt",
      "modifier": "canny",
      "version": "sdxl_base_v0.9",
      "type": "controlnet",
      "transformer_blocks": [
        0,
        0,
        1,
        1
      ]
    },
    {
      "name": "Color Palette (SD v1.x, T2I Adapter)",
      "file": "t2iadapter_color_1.x_f16.ckpt",
      "modifier": "color",
      "version": "v1",
      "type": "t2iadapter"
    },
    {
      "name": "Depth Map (SD v1.x, ControlNet 1.0)",
      "file": "controlnet_depth_1.x_f16.ckpt",
      "modifier": "depth",
      "version": "v1",
      "type": "controlnet",
      "deprecated": true
    },
    {
      "name": "Depth Map (SD v1.x, ControlNet 1.1)",
      "file": "controlnet_depth_1.x_v1.1_f16.ckpt",
      "modifier": "depth",
      "version": "v1",
      "type": "controlnet"
    },
    {
      "name": "Depth Map (SD v1.x, T2I Adapter)",
      "file": "t2iadapter_depth_1.x_f16.ckpt",
      "modifier": "depth",
      "version": "v1",
      "type": "t2iadapter"
    },
    {
      "name": "Depth Map (SD v2.x, ControlNet)",
      "file": "controlnet_depth_2.x_f16.ckpt",
      "modifier": "depth",
      "version": "v2",
      "type": "controlnet"
    },
    {
      "name": "Depth Map (SDXL, ControlNet, Diffusers 1.0 Mid)",
      "file": "controlnet_depth_sdxl_v1.0_mid_f16.ckpt",
      "modifier": "depth",
      "version": "sdxl_base_v0.9",
      "type": "controlnet",
      "transformer_blocks": [
        0,
        0,
        1,
        1
      ]
    },
    {
      "name": "Inpainting (SD v1.x, ControlNet 1.1)",
      "file": "controlnet_inpaint_1.x_v1.1_f16.ckpt",
      "modifier": "inpaint",
      "version": "v1",
      "type": "controlnet"
    },
    {
      "name": "Instruct Pix2Pix (SD v1.x, ControlNet 1.1)",
      "file": "controlnet_ip2p_1.x_v1.1_f16.ckpt",
      "modifier": "ip2p",
      "version": "v1",
      "type": "controlnet"
    },
    {
      "name": "IP Adapter Full Face (SD v1.x)",
      "file": "ip_adapter_full_face_sd_v1.x_open_clip_h14_f16.ckpt",
      "modifier": "shuffle",
      "version": "v1",
      "type": "ipadapterfull",
      "image_encoder": "open_clip_vit_h14_vision_model_f16.ckpt"
    },
    {
      "name": "IP Adapter Plus (SD v1.x)",
      "file": "ip_adapter_plus_sd_v1.x_open_clip_h14_f16.ckpt",
      "modifier": "shuffle",
      "version": "v1",
      "type": "ipadapterplus",
      "image_encoder": "open_clip_vit_h14_vision_model_f16.ckpt"
    },
    {
      "name": "IP Adapter Plus (SDXL Base)",
      "file": "ip_adapter_plus_xl_base_open_clip_h14_f16.ckpt",
      "modifier": "shuffle",
      "version": "sdxl_base_v0.9",
      "type": "ipadapterplus",
      "image_encoder": "open_clip_vit_h14_vision_model_f16.ckpt"
    },
    {
      "name": "IP Adapter Plus Face (SD v1.x)",
      "file": "ip_adapter_plus_face_sd_v1.x_open_clip_h14_f16.ckpt",
      "modifier": "shuffle",
      "version": "v1",
      "type": "ipadapterplus",
      "image_encoder": "open_clip_vit_h14_vision_model_f16.ckpt"
    },
    {
      "name": "IP Adapter Plus Face (SDXL Base)",
      "file": "ip_adapter_plus_face_xl_base_open_clip_h14_f16.ckpt",
      "modifier": "shuffle",
      "version": "sdxl_base_v0.9",
      "type": "ipadapterplus",
      "image_encoder": "open_clip_vit_h14_vision_model_f16.ckpt"
    },
    {
      "name": "LineArt (SD v1.x, ControlNet 1.1)",
      "file": "controlnet_lineart_1.x_v1.1_f16.ckpt",
      "modifier": "lineart",
      "version": "v1",
      "type": "controlnet"
    },
    {
      "name": "LineArt Anime (SD v1.x, ControlNet 1.1)",
      "file": "controlnet_lineart_anime_1.x_v1.1_f16.ckpt",
      "modifier": "lineart",
      "version": "v1",
      "type": "controlnet"
    },
    {
      "name": "MLSD Hough Map (SD v1.x, ControlNet 1.1)",
      "file": "controlnet_mlsd_1.x_v1.1_f16.ckpt",
      "modifier": "mlsd",
      "version": "v1",
      "type": "controlnet"
    },
    {
      "name": "Normal Map (SD v1.x, ControlNet 1.1)",
      "file": "controlnet_normalbae_1.x_v1.1_f16.ckpt",
      "modifier": "normalbae",
      "version": "v1",
      "type": "controlnet"
    },
    {
      "name": "Pose (SD v1.x, ControlNet 1.0)",
      "file": "controlnet_openpose_1.x_f16.ckpt",
      "modifier": "pose",
      "version": "v1",
      "type": "controlnet",
      "deprecated": true
    },
    {
      "name": "Pose (SD v1.x, ControlNet 1.1)",
      "file": "controlnet_openpose_1.x_v1.1_f16.ckpt",
      "modifier": "pose",
      "version": "v1",
      "type": "controlnet"
    },
    {
      "name": "Pose (SD v1.x, T2I Adapter)",
      "file": "t2iadapter_openpose_1.x_f16.ckpt",
      "modifier": "pose",
      "version": "v1",
      "type": "t2iadapter"
    },
    {
      "name": "Pose (SD v2.x, ControlNet)",
      "file": "controlnet_openpose_2.x_f16.ckpt",
      "modifier": "pose",
      "version": "v2",
      "type": "controlnet"
    },
    {
      "name": "QR Code (SD v1.x, ControlNet Monster 2.0)",
      "file": "controlnet_qr_code_monster_1.x_v2.0_f16.ckpt",
      "modifier": "scribble",
      "version": "v1",
      "type": "controlnet"
    },
    {
      "name": "Scribble (SD v1.x, ControlNet 1.0)",
      "file": "controlnet_scribble_1.x_f16.ckpt",
      "modifier": "scribble",
      "version": "v1",
      "type": "controlnet",
      "preprocessor": "hed_f16.ckpt",
      "deprecated": true
    },
    {
      "name": "Scribble (SD v1.x, ControlNet 1.1)",
      "file": "controlnet_scribble_1.x_v1.1_f16.ckpt",
      "modifier": "scribble",
      "version": "v1",
      "type": "controlnet",
      "preprocessor": "hed_f16.ckpt"
    },
    {
      "name": "Scribble (SD v1.x, T2I Adapter)",
      "file": "t2iadapter_sketch_1.x_f16.ckpt",
      "modifier": "scribble",
      "version": "v1",
      "type": "t2iadapter"
    },
    {
      "name": "Scribble (SD v2.x, ControlNet)",
      "file": "controlnet_scribble_2.x_f16.ckpt",
      "modifier": "scribble",
      "version": "v2",
      "type": "controlnet",
      "preprocessor": "hed_f16.ckpt"
    },
    {
      "name": "Segmentation (SD v1.x, ControlNet 1.1)",
      "file": "controlnet_seg_1.x_v1.1_f16.ckpt",
      "modifier": "seg",
      "version": "v1",
      "type": "controlnet"
    },
    {
      "name": "Shuffle (SD v1.x, ControlNet 1.1)",
      "file": "controlnet_shuffle_1.x_v1.1_f16.ckpt",
      "modifier": "shuffle",
      "version": "v1",
      "type": "controlnet",
      "global_average_pooling": true
    },
    {
      "name": "Soft Edge (SD v1.x, ControlNet 1.1)",
      "file": "controlnet_softedge_1.x_v1.1_f16.ckpt",
      "modifier": "softedge",
      "version": "v1",
      "type": "controlnet",
      "preprocessor": "hed_f16.ckpt"
    },
    {
      "name": "Tile (SD v1.x, ControlNet 1.1)",
      "file": "controlnet_tile_1.x_v1.1_f16.ckpt",
      "modifier": "tile",
      "version": "v1",
      "type": "controlnet"
    },
    {
      "name": "VACE (Wan 2.1, 1.3B)",
      "file": "wan_v2.1_1.3b_vace_480p_f16.ckpt",
      "modifier": "shuffle",
      "version": "wan_v2.1_1.3b",
      "type": "controlnet"
    },
    {
      "name": "VACE (Wan 2.1, 1.3B) (8-bit)",
      "file": "wan_v2.1_1.3b_vace_480p_q8p.ckpt",
      "modifier": "shuffle",
      "version": "wan_v2.1_1.3b",
      "type": "controlnet"
    },
    {
      "name": "VACE (Wan 2.1, 14B)",
      "file": "wan_v2.1_14b_vace_720p_f16.ckpt",
      "modifier": "shuffle",
      "version": "wan_v2.1_14b",
      "type": "controlnet"
    },
    {
      "name": "VACE (Wan 2.1, 14B) (8-bit)",
      "file": "wan_v2.1_14b_vace_720p_q8p.ckpt",
      "modifier": "shuffle",
      "version": "wan_v2.1_14b",
      "type": "controlnet"
    }
  ],
  "textualInversions": [],
  "upscalers": []
}